Title;Authors;Year;Venue;DOI;Abstract;Selected?;Exclusion Criteria
Risk analysis and control of large artificial intelligence models;Li, Shilong and Zhang, Li and Gu, Mengchen and Chan, Jian and Wu, Qi;2024;Proceedings of the 2024 International Conference on Cloud Computing and Big Data;https://doi.org/10.1145/3695080.3695149;With the rapid development of artificial intelligence(AI) technology, large AI models have demonstrated immense potential and value in numerous fields. However, the security risks associated with large AI models have also increasingly garnered widespread attention. This paper first outlines the development status and characteristics of large AI models and provides a detailed classification and discussion of the security risks. Next, the paper analyzes the causes of these risks, delving into technical, talent, and policy perspectives. Lastly, the paper proposes countermeasures for the secure development of large AI models, including improving technical regulation, enhancing policy regulations, strengthening talent cultivation, and reinforcing international cooperation, with the aim of providing a theoretical reference for the healthy development of large AI models.;No;lacks practical relevance to organizations
Artificial Intelligence Readiness in Africa: Status Quo and Future Research;Isagah, Tupokigwe and Ben Dhaou, Soumaya I.;2024;Proceedings of the 17th International Conference on Theory and Practice of Electronic Governance;https://doi.org/10.1145/3680127.3680199;Artificial Intelligence (AI) technologies have the potential to accelerate economic growth and promote sustainable development in developing countries. Particularly in Africa, the technology is expected to solve societal challenges, leapfrog, and promote sustainable communities. The literature presents examples of AI use cases in Africa to exemplify the opportunities, benefits, challenges, and risks of AI. Also, available reports highlight similar narratives with recommendations to improve the status quo. While such recommendations are essential, assessing what actions have already been implemented toward AI adoption is equally important. Still, more research is needed to showcase a holistic view of efforts taken by African countries to prepare for planning and implementing AI responsibly. This paper bridges the gap by examining the readiness of AI adoption in African countries from the literature using the Technology-Organization-Environment (TOE) framework. Findings reveal the need for further initiatives to realize the positive outcomes of AI technologies. The paper concludes by proposing the research areas that require further exploration from the context of African countries.;No;lacks practical relevance to organizations
Positioning Paradata: A Conceptual Frame for AI Processual Documentation in Archives and Recordkeeping Contexts;Cameron, Scott and Franks, Pat and Hamidzadeh, Babak;2023;J. Comput. Cult. Herit.;https://doi.org/10.1145/3594728;"The emergence of sophisticated Artificial Intelligence (AI) and machine learning tools poses a challenge to archives and records professionals, who are accustomed to understanding and documenting the activities of human agents rather than the often-opaque processes of sophisticated AI functioning. Preliminary work has proposed the term paradata to describe the unique documentation needs that emerge for archivists using AI tools to process records in their collections. For the purposes of archivists working with AI, paradata is conceptualized here as information recorded and preserved about records’ processing with AI tools; it is a category of data that is defined both by its relationship with other datasets and by the documentary purpose it serves. This article surveys relevant literature across three contexts to scope the relevant scholarship that archivists may draw upon to develop appropriate AI documentation practices. From the statistical social sciences and the visual heritage fields, the article discusses existing definitions of paradata and its ambiguous, often contextually dependent relationship with existing metadata categories. Approaching the problem from a sociotechnical perspective, literature on Explainable Artificial Intelligence (XAI) insists pointedly that explainability be attuned to specific users’ stated needs—needs that archivists may better articulate using the framework of paradata. Most importantly, the article situates AI as a challenge to accountability, transparency, and impartiality in archives by introducing an unfamiliar non-human agency, one that pushes the limits of existing archival practice and demands the development of new concepts and vocabularies to shape future technological and methodological developments in archives.";No;lacks practical relevance to organizations
The use and theoretical support of emerging technologies for citizen participation in cities. A Systematic Literature Review in DGRL;Rodr\'{\i;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657255;"This paper examines the use and theoretical support underlying the implementation of emerging technologies for citizen participation in cities. Based on a systematic literature review in the main database of digital government (Digital Government Research Library -DGRL-), we analyze the evidence drawn by prior research concerning the situation, theoretical support, and use of these disruptive technologies in citizen participation. We seek to contribute to prior research on arising a critical debate about the initial implementation of these technologies for promoting collaborative models of governance and providing future research directions to advance&nbsp;in&nbsp;this&nbsp;topic.";No;lacks practical relevance to organizations
Ethics of AI: A Systematic Literature Review of Principles and Challenges;Khan, Arif Ali and Badshah, Sher and Liang, Peng and Waseem, Muhammad and Khan, Bilal and Ahmad, Aakash and Fahmideh, Mahdi and Niazi, Mahmood and Akbar, Muhammad Azeem;2022;Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering;https://doi.org/10.1145/3530019.3531329;Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers, and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assesses the ethical capabilities of AI systems and provides best practices for further improvements.;No;lacks practical relevance to organizations
Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance;Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel;2022;Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society;https://doi.org/10.1145/3514094.3534181;Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.;No;lacks practical relevance to organizations
What About the Data? A Mapping Study on Data Engineering for AI Systems;Heck, Petra;2024;Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI;https://doi.org/10.1145/3644815.3644954;AI systems cannot exist without data. Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so. There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects. But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part. In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering. We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities. We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented. We end by an overall discussion of the papers with implications for practitioners and researchers. This paper creates an overview of the body of knowledge on data engineering for AI. This overview is useful for practitioners to identify solutions and best practices as well as for researchers to identify gaps.;No;lacks practical relevance to organizations
Mapping the individual, social and biospheric impacts of Foundation Models;Dom\'{\i;2024;Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency;https://doi.org/10.1145/3630106.3658939;Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.;No;lacks practical relevance to organizations
Regulating the machine: An exploratory study of US state legislations addressing Artificial Intelligence, 2019-2023;DePaula, Nic and Gao, Lu and Mellouli, Sehl and Luna-Reyes, Luis F. and Harrison, Teresa M.;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657148;"Artificial Intelligence (AI) poses transformative and disruptive challenges for democracy, for policy makers, and for government agencies. While various policy initiatives around the world seek to regulate AI, in the United States (US) federal government there is no sign of a comprehensive AI law, and few legal measures to enable or restrict AI have been proposed and passed. However, states across the US are active in attempting to address issues related to AI and have proposed hundreds of legislations related to AI in the past few years. In this paper, we examined what these legislations have sought to accomplish in relation to AI, and the potential impacts for the public in general and for public administration in particular. From a preliminary and descriptive analysis of all US state legislations related to AI passed from 2019 to 2023, we show how these legislations are addressing AI in terms of: (1) the types of legislations adopted or enacted; (2) the definitions of AI and associated technologies given; (3) the sectors and domains principally addressed in AI legislations; (4) the private sector and government actions directed by the legislations; and (5) how ethical and economic considerations are addressed. We generally found a lack of definition of AI, and associated technologies mentioned are rarely specific. Many of the laws create commissions or task forces to study AI, addressing the various practical and ethical issues related to AI. Legislations have created some regulations and support for industry, and have directed government agencies to identify existing AI capabilities and how AI may be employed in their agencies and jurisdictions. Considerable emphasis has been placed on issues of bias and discrimination, as well education and economic investment in AI, although unevenly distributed across states. We summarize and discuss these results in relation to existing literature and make some recommendations on how state legislatures may better address AI in the future.";No;lacks practical relevance to organizations
Hitting the Triple Bottom Line: Widening the HCI Approach to Sustainability;Scuri, Sabrina and Ferreira, Marta and Jardim Nunes, Nuno and Nisi, Valentina and Mulligan, Cathy;2022;Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3491102.3517518;Sustainable Development (SD) in its dimensions – environment, economy, and society – is a growing area of concern within the HCI community. This paper advances a systematic literature review on sustainability across the Sustainable Human-Computer Interaction (SHCI) body of work. The papers were classified according to the Triple Bottom Line (TBL) framework to understand how the pillars of SD play into the HCI discourse on sustainability. The economic angle was identified as a gap in SHCI literature. To meet the TBL of SD, however, a balance needs to be sought across all ‘lines’. In this paper, we propose that HCI can advance the discussion and the understanding of the economic concepts around sustainability through taking a sociology perspective on the economic angle of the TBL. We sustain this claim by discussing economic concepts and the role that digital can play in redefining the established foundations of our economic system.;No;lacks practical relevance to organizations
An Advance Review of Urban-AI and Ethical Considerations;Mirindi, Derrick and Sinkhonde, David and Mirindi, Frederic;2024;Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI;https://doi.org/10.1145/3681780.3697246;The rapid digitization of urban infrastructure and the availability of urban data have created opportunities for developing and using artificial intelligence (AI), machine learning (ML), and deep learning (DL) algorithms to address cities' difficult problems. This research offers a thorough evaluation of state-of-the-art AI, ML, and DL algorithms in urban artificial intelligence (AI). This encompasses land use, energy control, public safety, and traffic forecasting. We investigated urban-AI algorithms, and results show that ML algorithms such as Random Forest (RF) can achieve 94% accuracy in urban growth prediction, while Support Vector Machines (SVMs) have demonstrated power in accurately classifying objects such as built-up areas and vegetation. On the other hand, DL algorithms such as Convolutional Neural Networks (CNNs) can attain 79% accuracy in satellite image classification, while Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network are useful in time-series prediction tasks such as traffic flow forecasting, urban air quality prediction, and energy consumption modeling. However, the limitations of these algorithms, particularly when dealing with large datasets, could potentially restrict their scalability in real-time applications. Furthermore, we have identified ethical considerations such as privacy and surveillance, algorithmic bias and fairness, transparency and interpretability, accountability and human oversight, social inclusion, and civic participation, all of which require attention. This has resulted in a variety of suggestions, including creating guidelines for using AI for urban performance to address ethical issues. Future research directions should focus on integrating AI with emerging technologies such as 5G and developing robust frameworks for responsible AI governance in smart cities.;No;lacks practical relevance to organizations
Who evaluates the algorithms? An overview of the algorithmic accountability ecosystem;Criado, J. Ignacio and Guevara-Gomez, Ariana;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657247;Algorithmic accountability is a concept that has been gaining interest in academic and professional circles, especially when considering the potential negative impacts of Artificial Intelligence in diverse scenarios. In this article, we present the results of a systematic literature review and in-depth interviews to advance the understanding of the algorithmic accountability definition and the dynamics within the ecosystem of such processes, with a focus on the role of the public sector. We also offer some practical recommendations for fostering an accountability ecosystem that accommodates the complexities of algorithmic systems, through multi-stakeholder collaboration, public regulation and oversight, and citizen participation.;No;lacks practical relevance to organizations
Public Value Principles for Secure and Trusted AI;Ganapati, Sukumar and Desouza, Kevin;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657086;"The objective of this paper is to establish the fundamental public value principles that should govern safe and trusted artificial intelligence (AI). Public value is a dynamic concept that encompasses several dimensions. AI itself has evolved quite rapidly in the last few years, especially with the swift escalation of Generative AI. Governments around the world are grappling with how to govern AI, just as technologists ring alarm bells about the future consequences of AI. Our paper extends the debate on AI governance that is focused on ethical values of beneficence to that of economic values of public good. Viewed as a public good, AI use is beyond the control of the creators. Towards this end, the paper examined AI policies in the United States and Europe. We postulate three principles from a public values perspective: (i) ensuring security and privacy of each individual (or entity); (ii) ensuring trust in AI systems is verifiable; and (iii) ensuring fair and balanced AI protocols, wherein the underlying components of data and algorithms are contestable and open to public debate.";No;lacks practical relevance to organizations
Exploring AI Futures Through Role Play;Avin, Shahar and Gruetzemacher, Ross and Fox, James;2020;Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society;https://doi.org/10.1145/3375627.3375817;We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.;No;lacks practical relevance to organizations
The impact of Artificial Intelligence on the Socioeconomic factors in the UAE;Hassanien, Ahmed Rabie Mohamed;2024;Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference;https://doi.org/10.1145/3639592.3639609;Artificial Intelligence (AI) is rapidly transforming industries worldwide, including the United Arab Emirates (UAE). As the UAE aims to become a leader in AI adoption and deployment, it is crucial to understand its impact on the country's socioeconomic factors. This research proposal aims to investigate the impact of AI on employment, wage inequality, productivity, innovation, and ethical implications in the UAE.The research will adopt a mixed-methods approach combining a systematic literature review and a quantitative survey to gather and assess data. The literature review will identify relevant literature to establish the research framework, while the survey will target leaders, employees, and end-users of organizations operating in the UAE across various sectors.A sample size of no less than 250 respondents who are predominantly leadership executives will be used, and the survey will consist of 30 questions, including demographic questions, and a 5-point Likert scale to measure respondents’ level of agreement or disagreement with the statements presented.Ethical considerations, including informed consent and anonymity, will be addressed. The findings from this research will contribute to a better understanding of the impact of AI on the socioeconomic factors in the UAE and help inform the development of appropriate policies and strategies for the responsible adoption and deployment of AI technologies in the country.The research proposal presents an exciting opportunity to explore a relevant and timely topic that will shed light on the potential impact of AI on the UAE's socioeconomic development.;No;lacks practical relevance to organizations
Ethical Tensions in Applications of AI for Addressing Human Trafficking: A Human Rights Perspective;Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy;2022;Proc. ACM Hum.-Comput. Interact.;https://doi.org/10.1145/3555186;"In the last two decades, human trafficking (where individuals are forcibly exploited for the profits of another) has seen increased attention from the artificial intelligence (AI) community. Clear focus on the ethical risks of this research is critical given that those risks are disproportionately born by already vulnerable populations. To understand and subsequently address these risks, we conducted a systematic literature review of computing research leveraging AI to combat human trafficking and apply a framework using principles from international human rights law to categorize ethical risks. This paper uncovers a number of ethical tensions including bias endemic in datasets, privacy risks stemming from data collection and reporting, and issues concerning potential misuse. We conclude by highlighting four suggestions for future research: broader use of participatory design; engaging with other forms of trafficking; developing best practices for harm prevention; and including transparent ethics disclosures in research. We find that there are significant gaps in what aspects of human trafficking researchers have focused on. Most research to date focuses on aiding criminal investigations in cases of sex trafficking, but more work is needed to support other anti-trafficking activities like supporting survivors, adequately address labor trafficking, and support more diverse survivor populations including transgender and nonbinary individuals.";No;lacks practical relevance to organizations
Fundamental Trends and Hot-Spots in Public Governance and Artificial Intelligence Research: A Bibliometric Study Analysis From 2013 to 2023;Lee, Siyuan;2024;Proceedings of the 5th International Conference on Computer Information and Big Data Applications;https://doi.org/10.1145/3671151.3671290;Background: AI has become an important theoretical and practical hotspot in public governance. This study aims to analyses the publication characteristics of AI in public governance research in terms of institutions, journals and partnerships, and to analyses the development trend of AI in public governance research. Methods: Publications regarding AI in public governance were retrieved from Web of Science core collection. Microsoft Excel 2010, VOSviewer were used to characterize the contributions of the authors, journals, and institutions. The trends, hot-spots and knowledge network were analyzed by Citespace and VOSviewer. Results: We identified 2525 papers between 2013 and 2023. The number of papers grows gradually until 2019, followed by a sharp increase between 2020 and 2023. The University of Oxford is the most active institution. Most of the papers in this field are published in Sustainability and IEEE Access. Luciano is the most productive author, and EUROPEAN COMMISSION is the most frequently co-cited author. The frontier topics are public sector and smart city. The basic themes of this field are “application scenarios, operation characteristics, algorithm characteristics, risk governance”. Conclusion: In the past three years, artificial intelligence and public governance research have attracted widespread attention. The research hotspots are gradually shifting to public service, decision-making and risk.;No;lacks practical relevance to organizations
AI Impact on Health Equity for Marginalized, Racial, and Ethnic Minorities;Iloanusi, Nchebe-Jah and Chun, Soon Ae;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657152;Predictive analytics technologies like machine learning, AI and Generative AI models like Large Language Models (LLMs), have garnered enthusiasm for their potential to improve healthcare services in smart cities. However, these rapidly developing intelligent agents that guides the healthcare decisions may also risk exacerbating health inequities along racial, ethnic, gender, and socioeconomic lines, reflecting systemic discrimination ingrained within healthcare practices. Flawed or injudiciously applied AI systems could improperly restrict opportunities and provide substandard care for minority groups by propagating historical patterns of prejudice encoded within limited training datasets. These advanced intelligent technologies can hinder the sustainable health solutions for smart cities. This study examines intelligent AI models and applications in healthcare settings, with a focus on assessing impacts on marginalized and disadvantaged populations. Comprehensive scholarly database searches identified 45 relevant studies investigating issues on algorithmic bias, lack of diverse training data, and discrimination risks linked to healthcare AI systems. The review finds most applications still lack adequate safeguards to prevent discrimination against vulnerable populations. Through the review of these systems, we propose an integrated inclusive smart health model that considers both technical interventions as well as broader participatory and ethical approaches. Realizing AI’s fullest potential to meaningfully advance health justice requires not only algorithmic adjustments to mitigate bias, and efforts to improve diversity of training data, transparent analysis frameworks, and best practices for ensuring just AI systems in healthcare, but also a human-centered commitment to thoughtful, inclusive development approaches that center the needs and priorities of communities impacted by health disparities from the outset.;No;lacks practical relevance to organizations
Towards a roadmap on software engineering for responsible AI;Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Xing, Zhenchang;2022;Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI;https://doi.org/10.1145/3522664.3528607;Although AI is transforming the world, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible AI have been issued recently. However, they are high level and difficult to put into practice. On the other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible AI systems, (ii) setting up the development processes incorporating process-oriented practices for responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques.;No;lacks practical relevance to organizations
Towards the regulation of Large Language Models (LLMs) and Generative AI use in the Brazilian Government: the case of a State Court of Accounts;Alves, Karine and Santos, Edney and Silva, Matheus Fidelis and Chaves, Ana Carolina and Fernandes, Jose Andre and Valenca, George and Brito, Kellyton;2024;Proceedings of the 17th International Conference on Theory and Practice of Electronic Governance;https://doi.org/10.1145/3680127.3680219;"The rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) has been highlighted by the rise of platforms like ChatGPT, which garnered a significant user base within a short span of time. This sudden surge has sparked both excitement and concern among the government and public bodies, which are struggling to establish regulations governing AI usage. In Brazil, for instance, the legal landscape for AI remains uncertain, leaving public organizations uncertain about the path forward. In light of this ambiguity, this paper aims to develop and implement a methodology for establishing AI regulations in the public service, with a specific emphasis and partnership with the State Court of Accounts. To achieve this, a comprehensive methodology with five stages was collaboratively developed: (i) research and analysis; (ii) define the content of the regulation; (iii) categorize and refine; (iv) write a draft proposal; and (v) discuss and validate with stakeholders. Notably, the initial stage involved extensive research encompassing academic literature, existing and pending legislation in Brazil, and global initiatives. As a result, a set of 4 principles, 13directives, 22 acceptable uses, and 25 best practices were delineated and validated. This work represents a pioneering approach to AI regulation within Brazilian public institutions, offering a clear and actionable methodology that can serve as a reference point for future regulatory initiatives.";No;lacks practical relevance to organizations
Mind the Gap: Towards an Understanding of Government Decision-Making based on Artificial Intelligence;Valle-Cruz, David and Garc\'{\i;2022;Proceedings of the 23rd Annual International Conference on Digital Government Research;https://doi.org/10.1145/3543434.3543445;Decision-making has become more critical for organizations in the 21st century. The citizens’ countless needs and the emerging problems (internal and external) faced by governments increase the complexity of government decisions worldwide. The research question guiding this attempt is: How is government decision-making grounded on artificial intelligence (AI)? Based on the PRISMA approach and empirical analysis of some international cases are adopted. The authors analyze different organizational and environmental factors, the objectives, benefits, and risks of AI-supported decision-making. The findings show an increasing interest in the research on government decision-making based on AI. Finally, there is the potential of AI to support decision-making for the benefit of citizens and public value generation, collaboratively between governments, industry, and society. Future work will further analyze AI-based decision-making in government in depth.;No;lacks practical relevance to organizations
More than an IT system in the government: The work divide challenges in human-AI coworking context;Huang, Hsini and Chen, Yen-Yu and Kuo, Nai-Ling and Hung, Mei-Jen;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657058;The study examines the collaboration between AI and humans in Taiwanese government agencies and the reasons for different modes of collaboration. Through interviews, four distinct patterns were identified based on the division of labor and the level of influence on citizens' rights. In cases where citizens' rights are minimally affected and the division of labor is independent, AI serves as a low-risk tool for routine tasks. For applications significantly affecting citizens' rights, especially those with mature AI technologies, government agencies tend to adopt a conservative approach, using AI as a reference or support rather than a primary decision-maker. The analysis also reveals instances where AI assists in tasks beyond its current capabilities, emphasizing the importance of Human-AI collaboration in resolving complex issues. Overall, the role of AI in Human-AI collaboration transcends traditional categorizations, showcasing nuanced relationships and varying degrees of collaboration across different contexts. Additionally, the study draws insights from past literature on ICTs' impact on bureaucratic systems, highlighting the evolving roles of public servants and potential challenges arising from technological advancements.;No;lacks practical relevance to organizations
Impact and barriers to AI in the public sector: the case of the State of Mexico;Millan-Vargas, Adrian Osiel and Sandoval-Almazan, Rodrigo and Valle-Cruz, David;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657249;The use and implementation of Artificial Intelligence (AI) tools for doing repetitive tasks in the public sector is a challenge, particularly in persuading bureaucrats. However, the potential benefits for citizens, such as improved process and services related to tax payments and basic services using machine learning or diffuse logic for decision making or logistic distribution, are significant. This research aims to understand the perceptions of public managers regarding the impact, functions, and barriers of AI in the context of a local government. A survey was conducted among 32 key public managers from the government of the State of Mexico in the central region to assess their perceptions of AI. The findings indicate that there is widespread concern among public administrators regarding high costs, suggesting the critical need to address financial issues to ensure sustainable implementation of AI. In terms of barriers, the results underscore the urgent necessity of addressing fundamental issues such as connectivity, financial resources, and technological capacity to enable effective integration of AI. This study is relevant as it identifies the key aspects of impact, functions, and barriers for the implementation of AI in a local government.;No;lacks practical relevance to organizations
A Design Space for Intelligent and Interactive Writing Assistants;Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David and Alghamdi, Emad A. and August, Tal and Bhat, Avinash and Choksi, Madiha Zahrah and Dutta, Senjuti and Guo, Jin L.C. and Hoque, Md Naimul and Kim, Yewon and Knight, Simon and Neshaei, Seyed Parsa and Shibani, Antonette and Shrivastava, Disha and Shroff, Lila and Sergeyuk, Agnia and Stark, Jessi and Sterman, Sarah and Wang, Sitong and Bosselut, Antoine and Buschek, Daniel and Chang, Joseph Chee and Chen, Sherol and Kreminski, Max and Park, Joonsuk and Pea, Roy and Rho, Eugenia Ha Rim and Shen, Zejiang and Siangliulue, Pao;2024;Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3613904.3642697;In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions and codes by systematically reviewing 115 papers, while leveraging the expertise of researchers in various disciplines. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the design of new writing assistants.;No;lacks practical relevance to organizations
Documenting Ethical Considerations in Open Source AI Models;Gao, Haoyu and Zahedi, Mansooreh and Treude, Christoph and Rosenstock, Sarita and Cheong, Marc;2024;Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement;https://doi.org/10.1145/3674805.3686679;Background: The development of AI-enabled software heavily depends on AI model documentation, such as model cards, due to different domain expertise between software engineers and model developers. From an ethical standpoint, AI model documentation conveys critical information on ethical considerations along with mitigation strategies for downstream developers to ensure the delivery of ethically compliant software. However, knowledge on such documentation practice remains scarce. Aims: The objective of our study is to investigate how developers document ethical aspects of open source AI models in practice, aiming at providing recommendations for future documentation endeavours. Method: We selected three sources of documentation on GitHub and Hugging Face, and developed a keyword set to identify ethics-related documents systematically. After filtering an initial set of 2,347 documents, we identified 265 relevant ones and performed thematic analysis to derive the themes of ethical considerations. Results: Six themes emerge, with the three largest ones being model behavioural risks, model use cases, and model risk mitigation. Conclusions: Our findings reveal that open source AI model documentation focuses on articulating ethical problem statements and use case restrictions. We further provide suggestions to various stakeholders for improving documentation practice regarding ethical considerations.;No;lacks practical relevance to organizations
Blaming Humans and Machines: What Shapes People’s Reactions to Algorithmic Harm;Lima, Gabriel and Grgi\'{c;2023;Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3544548.3580953;Artificial intelligence (AI) systems can cause harm to people. This research examines how individuals react to such harm through the lens of blame. Building upon research suggesting that people blame AI systems, we investigated how several factors influence people’s reactive attitudes towards machines, designers, and users. The results of three studies (N = 1,153) indicate differences in how blame is attributed to these actors. Whether AI systems were explainable did not impact blame directed at them, their developers, and their users. Considerations about fairness and harmfulness increased blame towards designers and users but had little to no effect on judgments of AI systems. Instead, what determined people’s reactive attitudes towards machines was whether people thought blaming them would be a suitable response to algorithmic harm. We discuss implications, such as how future decisions about including AI systems in the social and moral spheres will shape laypeople’s reactions to AI-caused harm.;No;lacks practical relevance to organizations
Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made;Vereschak, Oleksandra and Alizadeh, Fatemeh and Bailly, Gilles and Caramiaux, Baptiste;2024;Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3613904.3642018;"Trust between humans and AI in the context of decision-making has acquired an important role in public policy, research and industry. In this context, Human-AI Trust has often been tackled from the lens of cognitive science and psychology, but lacks insights from the stakeholders involved. In this paper, we conducted semi-structured interviews with 7 AI practitioners and 7 decision subjects from various decision domains. We found that 1) interviewees identified the prerequisites for the existence of trust and distinguish trust from trustworthiness, reliance, and compliance; 2) trust in AI-integrated systems is strongly influenced by other human actors, more than the system’s features; 3) the role of Human-AI trust factors is stakeholder-dependent. These results provide clues for the design of Human-AI interactions in which trust plays a major role, as well as outline new research directions in Human-AI Trust.";No;lacks practical relevance to organizations
Designing Interactive Explainable AI Tools for Algorithmic Literacy and Transparency;Bhat, Maalvika and Long, Duri;2024;Proceedings of the 2024 ACM Designing Interactive Systems Conference;https://doi.org/10.1145/3643834.3660722;As artificial intelligence (AI) increasingly permeates everyday life, there is a growing need for public understanding of AI’s underlying principles. Existing educational interventions and explainable AI (XAI) tools cater mainly to children or adult experts. In this paper, we present three interactive web-based tools to foster AI learning among adults without technical backgrounds. Designed according to learning sciences and user-centered design principles, these tools simplify complex AI concepts like edge detection, confidence thresholds, and sensitivity, making AI more understandable for beginners and facilitating reflection on ethical issues. We present results from a mixed-methods evaluation of the tools with 42 participants. Results show heightened familiarity and confidence in AI concepts. Our qualitative analysis additionally reveals common interaction patterns amongst participants. This paper offers both a design contribution to the AI education and XAI communities and emergent interaction patterns to support the design of transparent and learner-centered AI for adult novices.;No;lacks practical relevance to organizations
Crowdsourcing Smart City: SmartTaipei Project;Liu, Helen K. and Guo, Yu-Wei and Chen, Liang-Yu;2023;Proceedings of the 24th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3598469.3598474;Increasingly governments engage the crowd through online platforms to generate innovation and utilize collaborative platforms for AI adoptions as a governance strategy. This study aims to investigate the factors associated with terminated and implemented smart city or AI related technologies projects through qualitative analysis. This study selected a setting based on the Taipei Smart City Industrial Field Experiment Pilot Program since 2016. We collected online achieved data of the smart city proposals from 2016 to April 2023 (N=295). Based on those achieved data with descriptions for termination and implementation, this paper discusses the opportunities, challenges, and suggestions for overcoming such challenges of the SmartTaipei Project. Given that governments do not provide financial support for smart city pilot projects, enterprises acquire opportunities to experiment with and thus improve their products or services. The successful experiment is conducive to research and development, business expansion, and even government procurement. However, despite the fact that the SmartTaipei proposals and projects have been discussed and modified several times before implementation, 41.7% of the projects were terminated due to several reasons. We classify the total number of AI technologies adopted across all projects. We also use the Technology Enactment Framework as the analysis framework to summarize the reason why nearly 41.7% of the project (N=123) were terminated. To address the challenges of building smart cities, this paper suggests that municipal governments (1) build up platforms for stakeholder participation and assess progressive results, (2) develop a comprehensive development strategy and enhance digital infrastructure, and (3) develop a convention for international exchange.;No;lacks practical relevance to organizations
“It is currently hodgepodge”: Examining AI/ML Practitioners’ Challenges during Co-production of Responsible AI Values;Varanasi, Rama Adithya and Goyal, Nitesh;2023;Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3544548.3580903;Recently, the AI/ML research community has indicated an urgent need to establish Responsible AI (RAI) values and practices as part of the AI/ML lifecycle. Several organizations and communities are responding to this call by sharing RAI guidelines. However, there are gaps in awareness, deliberation, and execution of such practices for multi-disciplinary ML practitioners. This work contributes to the discussion by unpacking co-production challenges faced by practitioners as they align their RAI values. We interviewed 23 individuals, across 10 organizations, tasked to ship AI/ML based products while upholding RAI norms and found that both top-down and bottom-up institutional structures create burden for different roles preventing them from upholding RAI values, a challenge that is further exacerbated when executing conflicted values. We share multiple value levers used as strategies by the practitioners to resolve their challenges. We end our paper with recommendations for inclusive and equitable RAI value-practices, creating supportive organizational structures and opportunities to further aid practitioners.;No;lacks practical relevance to organizations
South Korean Public Value Coproduction Towards ‘AI for Humanity’: A Synergy of Sociocultural Norms and Multistakeholder Deliberation in Bridging the Design and Implementation of National AI Ethics Guidelines;Ha, You Jeen;2022;Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency;https://doi.org/10.1145/3531146.3533091;"As emerging technologies such as Big Data, Artificial Intelligence (AI), robotics, and the Internet of Things (IoT) pose fundamental challenges for global and domestic technological governance, the ‘Fourth Industrial Revolution’ (4IR) comes to the fore with AI as a frontrunner, generating discussions on the ethical elements of AI amongst key stakeholder groups, such as government, academia, industry, and civil society. However, in recent AI ethics and governance scholarship, AI ethics design appears to be divorced from AI ethics implementation, an implicit partition that results in two separate matters of theory and practice, respectively, and thus invokes efforts to bridge the ‘gap’ between the two. Such a partition potentially overcomplicates the discussion surrounding AI ethics and limits its productivity. This paper thus presents South Korea's people-centered ‘National Guidelines for Artificial Intelligence Ethics’ (??????????; ‘Guidelines’) and their development under the Moon administration as a case study that can help readers conceptualize AI ethics design and implementation as a continuous process rather than a partitioned one. From a public value perspective, the case study examines the Guidelines and the multistakeholder policymaking infrastructure that serves as the foundation for both the Guidelines’ design and implementation. This examination draws from literature in AI ethics and governance, public management and administration, and Korean policy and cultural studies as well as government and public documents alongside 9 interviews with members from the four stakeholder groups that collectively designed and continue to deliberate upon the Guidelines. Further, the study specifically focuses on (i) identifying public values that were highlighted by the Guidelines, (ii) investigating how such values reflect prevalent Korean sociocultural norms, and (iii) exploring how these values, in a way made possible by Korean sociocultural norms and policymaking, have been negotiated amongst the four stakeholder groups in a democratic public sphere to be ultimately incorporated into the Guidelines and prepared for implementation. This paper hopes to contribute to theory-building in AI ethics and provide a point of comparison in the international stage for future research concerning AI ethics design and implementation.";No;lacks practical relevance to organizations
"""Something Fast and Cheap"" or ""A Core Element of Building Trust""? - AI Auditing Professionals' Perspectives on Trust in AI";Lassiter, Tina B. and Fleischmann, Kenneth R.;2024;Proc. ACM Hum.-Comput. Interact.;https://doi.org/10.1145/3686963;Artificial Intelligence (AI) auditing is a relatively new area of work. Currently, there is a lack of uniform standards and regulation. As a result, the AI auditing ecosystem is very diverse, and AI auditing professionals use a variety of different auditing methods. So far, little is known about how AI auditors approach the concept of trust in AI through AI audits, in particular regarding the trust of users. This paper reports findings from interviews with 19 AI auditing stakeholders to understand how AI auditing professionals seek to create calibrated trust in AI tools and AI audits. Themes identified included the AI auditing ecosystem, participants' experiences with AI auditing, and trust in AI audits and AI. The paper adds to the existing research on trust in AI and trustworthiness in AI by adding perspectives of key stakeholders regarding trust in AI Audits by users as an essential and currently less explored part of the trust in AI research. This paper shows how information asymmetry in respect to AI audits can decrease the value of audits for users and consequently their trust in AI systems. Study participants suggest key elements for rebuilding trust and suggest recommendations for the AI auditing industry, such as monitoring of auditors and effective communication about AI audits.;No;lacks practical relevance to organizations
Evaluating the impact of artificial intelligence technologies in public services: towards an assessment framework;van Noordt, Colin and Misuraca, Gianluca;2020;Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance;https://doi.org/10.1145/3428502.3428504;Many governments are exploring applications of AI technologies to improve their public services. While AI has the potential to radically improve governmental processes, services and policy, limited studies empirically validate the effects of the use of AI. In this research paper, a first discussion on the development of a conceptual framework to research more rigorously the effects of AI in government is proposed. The proposed elements of the framework build upon the current understanding of the drivers influencing adoption of AI and takes into account the need for complementary organisational changes for increasing impact. The model follows a public value approach to understand the possible impact of AI on both the internal mechanism of the organisation, public service quality and broader societal effects.;No;lacks practical relevance to organizations
Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services;Karusala, Naveena and Upadhyay, Sohini and Veeraraghavan, Rajesh and Gajos, Krzysztof Z.;2024;Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3613904.3641898;Policymakers have established that the ability to contest decisions made by or with algorithms is core to responsible artificial intelligence (AI). However, there has been a disconnect between research on contestability of algorithms, and what the situated practice of contestation looks like in contexts across the world, especially amongst communities on the margins. We address this gap through a qualitative study of follow-up and contestation in accessing public services for land ownership in rural India and affordable housing in the urban United States. We find there are significant barriers to exercising rights and contesting decisions, which intermediaries like NGO workers or lawyers work with communities to address. We draw on the notion of accompaniment in global health to highlight the open-ended work required to support people in navigating violent social systems. We discuss the implications of our findings for key aspects of contestability, including building capacity for contestation, human review, and the role of explanations. We also discuss how sociotechnical systems of algorithmic decision-making can embody accompaniment by taking on a higher burden of preventing denials and enabling contestation.;No;lacks practical relevance to organizations
Legal Provocations for HCI in the Design and Development of Trustworthy Autonomous Systems;Urquhart, Lachlan D. and McGarry, Glenn and Crabtree, Andy;2022;Nordic Human-Computer Interaction Conference;https://doi.org/10.1145/3546155.3546690;We propose a series of legal provocations emerging from the proposed European Union Artificial Intelligence Act 2021 (AIA) and explore how they open up new possibilities for HCI in the design and development of trustworthy autonomous systems. The AIA continues the ‘by design’ trend seen in recent EU regulation of emerging technologies. The AIA targets AI developments that pose risks to society and citizens’ fundamental rights, introducing mandatory design and development requirements for high-risk AI systems (HRAIS). These requirements regulate different stages of the AI development cycle including ensuring data quality and governance strategies, mandating testing of systems, ensuring appropriate risk management, designing for human oversight, and creating technical documentation. These requirements open up new opportunities for HCI that reach beyond established concerns with the ethics and explainability of AI and situate AI development in human-centered processes and methods of design to enable compliance with regulation and foster societal trust in AI.;No;lacks practical relevance to organizations
The Who in XAI: How AI Background Shapes Perceptions of AI Explanations;Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O;2024;Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3613904.3642474;Explainability of AI systems is critical for users to take informed actions. Understanding who opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups—people with and without AI background—perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.;No;lacks practical relevance to organizations
Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models;Pafla, Marvin and Larson, Kate and Hancock, Mark;2024;Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3613904.3642934;The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N = 40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N = 136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.;No;lacks practical relevance to organizations
Conversational User Interfaces on Mobile Devices: Survey;Jaber, Razan and McMillan, Donald;2020;Proceedings of the 2nd Conference on Conversational User Interfaces;https://doi.org/10.1145/3405755.3406130;"Conversational User Interfaces (CUI) on mobile devices are the most accessible and widespread examples of voice-based interaction in the wild. This paper presents a survey of mobile conversation user interface research since the commercial deployment of Apple's Siri, the first readily available consumer CUI. We present and discuss Text Entry &amp; Typing, Application Control, Speech Analysis, Conversational Agents, Spoken Output, &amp; Probes as the prevalent themes of research in this area. We also discuss this body of work in relation to the domains of Health &amp; Well-being, Education, Games, and Transportation. We conclude this paper with a discussion on Multi-modal CUIs, Conversational Repair, and the implications for CUIs of greater access to the context of use.";No;lacks practical relevance to organizations
A study of UX practitioners roles in designing real-world, enterprise ML systems;Zdanowska, Sabah and Taylor, Alex S;2022;Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems;https://doi.org/10.1145/3491102.3517607;Opportunities for AI and machine learning (ML) are vast in current interactive systems development. However, comparatively little is known about how functionality for the system behind the interface is designed and how design methodologies such as user-centered design have influence. This research focuses on how interdisciplinary teams that include UX practitioners design real-world enterprise ML systems outside of big technology companies. We conducted a survey with product managers, and interviews with interdisciplinary teams and individual UX practitioners. The findings show that nontechnical UX practitioners are highly capable in designing AI/ML systems. In addition to applying UX and interaction design expertise to make decisions regarding functionality, they employ skills that aid collaboration across interdisciplinary teams. However, our findings suggest existing HCI design techniques such as prototyping and simulating complexity of enterprise ML systems are insufficient. We propose adaptations to design practices and conclude that some existing research should be reconsidered.;No;lacks practical relevance to organizations
Artificial Intelligence Law for Malaysia;Mohamed Yusof, Nor Ashikin and Sazrina Saimy, Intan and Salleh, Siti Hasliah and Badrul Hisham, Amirah 'Aisha and Mustafa, Wan Azani and Alkafaji, Hassan;2023;International Conference for Technological Engineering and its Applications in Sustainable Development (ICTEASD);https://doi.org/10.1109/ICTEASD57136.2023.10584852;"This study examines the proposed artificial intelligence (AI) law in Malaysia, exploring its distinctive approach within the global landscape. While adopting the established ""AI Principles,"" it rebrands them as ""Seven Responsible AI Principles"" and introduces the novel concept of ""pursuit of happiness"" as one of the core principles. This reflects Malaysia's commitment to forging its own path in AI governance, aligning with global standards while embedding unique cultural values and aspirations as expounded by the Federal Constitution and Rukun Negara Principles. Employing a qualitative approach, data was gathered through literature review and Focus Group Discussions (FGDs) with 80 participants representing diverse Quad Helix groups. Thematic and content analysis, along with triangulation, allowed for identification of key themes related to the perceived uniqueness of the proposed law and its alignment with national values. This research contributes significantly by shedding light on Malaysia's innovative approach to AI governance. By prioritizing national values and happiness alongside adherence to global standards, the proposed law has the potential to serve as a model for other nations seeking responsible and culturally sensitive AI regulations. This study offers valuable insights into the early stages of Malaysia's AI journey, paving the way for further discussions and research as the law progresses towards implementation.";No;lacks practical relevance to organizations
Data Science Failure: A Literature Review;Lahiri, Sucheta and Saltz, Jeff ;2023;2023 IEEE International Conference on Big Data (BigData);https://doi.ieeecomputersociety.org/10.1109/BigData59044.2023.10386265;"Data science is a multifaceted field that integrates statistics, computer science, social science, and other domains to generate valuable insights from data. Despite unprecedented development, many data science projects fail to achieve desired outcomes. This paper presents a work-in-progress systematic literature review of grey literature to explore the opinions of industry practitioners on data science failure. Specifically, this study reviews trade journals, news articles, blogs, and industry reports published from 2018-2023 to identify common data science failure themes outside of traditional academic literature. Initial findings reveal that technical, process, people, financial, and organizational frictions frequently undermine data science projects. Furthermore, risks related to AI governance, ethical considerations, CRM strategies, data quality, access, and team skills also contribute to data science failure. The analysis highlights the contextual nature of “failure,” emphasizing the importance of critical thinking that must align with data science goals and business needs. In short, the results suggest that grey literature provides unique perspectives into data science failure, which can be complementary to peer-reviewed scholarship.}, 
";No;lacks practical relevance to organizations
Formulating Analytical Governance Frameworks: An Integration of Data and AI Governance Approaches;Kanying, Thanika and Thammaboosadee, Sotarat and Chuckpaiwong, Rojjalak;2023;Proceedings of the 13th International Conference on Advances in Information Technology;https://doi.org/10.1145/3628454.3628461;This study underscores the importance of standard data analytics governance within organizations to leverage the potential of increasing volumes of big data. Even though Thailand has operative frameworks for data governance and AI governance, there lacks a concrete framework specifically for analytics governance. To fill this void, the research utilizes the Data Governance Framework from the Digital Government Development Agency (DGA), the Artificial Intelligence Governance for e-Business and Digital Services from the Electronic Transactions Development Agency (ETDA), and the Data Management Capability Assessment Model (DCAM) within the Analytics Management section. The design and evaluation of the analytics governance framework are carried out using feedback from experts via a questionnaire. The ultimate objective of this study is to pinpoint the relevant components necessary for formulating an analytics governance framework in organizations in Thailand, which have already implemented data governance.;No;Not a secondary study
U.S. Public Opinion on the Governance of Artificial Intelligence;Zhang, Baobao and Dafoe, Allan;2020;Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society;https://doi.org/10.1145/3375627.3375827;Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.;No;Not a secondary study
An Artificial Intelligence Definition and Classification Framework for Public Sector Applications;Ballester, Omar;2021;Proceedings of the 22nd Annual International Conference on Digital Government Research;https://doi.org/10.1145/3463677.3463709;Artificial Intelligence (AI) currently holds the spotlight amongst the interdisciplinary research fields that transcend the realm of academia. AI methods are enabled by a wide variety of technologies, methods and data, presenting a number of opportunities and challenges in a public decision-making context that differ by context of application. While the regulatory landscape surrounding AI governance is a pressing problem for scholars, the literature has often overlooked the role of the government as a user of AI. This article presents a classification framework for AI applications that facilitates the work in public sector (in the role of user) deployment. With our framework, we try to provide a lens to different deployment-related questions.;No;Not a secondary study
A Roadmap of Explainable Artificial Intelligence: Explain to Whom, When, What and How?;Mostafa Haghir Chehreghani;2024;https://doi.org/10.1145/3702004;ACM Computing Surveys;This article presents a roadmap for explainable AI (XAI), detailing stakeholder needs, explanation goals, and matching methods throughout the AI lifecycle, offering guidance for researchers and practitioners.;Yes;
A Systematic Literature Review on AI-Based Recommendation Systems and Their Ethical Considerations;Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li;2024;https://doi.org/10.1109/ACCESS.2024.3451054;ACM Transactions on Knowledge Discovery from Data;This paper explores the pivotal role of trust in the widespread application of Artificial Intelligence (AI) across various domains. We review AI applications in sectors like energy, healthcare, and autonomous vehicles and discuss the crisis of human trust they face. This paper introduces a novel framework that delineates the relationship between AI transparency and user trust, highlighting specific industry applications. Through a systematic review of recent literature, we first delve into factors such as emotional response, acceptance, transparency, accuracy, and interpretability that shape human trust in AI. We then underscore the necessity of ethical AI practices and highlight the importance of regulatory measures to keep pace with technological advancements. Finally, we propose strategic measures for maintaining trust in AI, focusing on stringent regulation, ethical authorization, and effective human-AI cooperation, thereby contributing to the ongoing discourse on AI trust.;Yes;
Developing an Ethical Regulatory Framework for Artificial Intelligence: Integrating Systematic Review, Thematic Analysis, and Multidisciplinary Theories;Wang, Jian and Huo, Yujia and Mahe, Jinli and Ge, Zongyuan and Liu, Zhangdaihong and Wang, Wenxin and Zhang, Lin;2024;IEEE Access;https://doi.org/10.1109/ACCESS.2024.3501332;Artificial intelligence (AI) ethics has emerged as a global discourse within both academic and policy spheres. However, translating these principles into concrete, real-world applications for AI development remains a pressing need and a significant challenge. This study aims to bridge the gap between principles and practice from a regulatory government perspective and promote best practices in AI governance. To this end, we developed the Ethical Regulatory Framework for AI (ERF-AI) to guide regulatory bodies in constructing mechanisms, including role setups, procedural configurations, and strategy design. The framework was developed through a systematic review, thematic analysis, and the integration of interdisciplinary concepts. A comprehensive search was conducted across four electronic databases (PubMed, IEEE Xplore, Web of Science, and Scopus) and four additional sources containing AI standards and guidelines from various countries and international organizations, focusing on studies published from 2014 to 2024. Thematic analysis identified and refined key themes from the included literature and integrated concepts from process control theory, computer science, organizational management, information technology, and behavioral psychology. This study adhered to the PRISMA guidelines and employed NVivo for thematic analysis. The resulting framework encompasses 23 themes, particularly emphasizing three feedback-loop processes: the ethical review process, the incentive and penalty process, and the mechanism improvement process, offering theoretical guidance for the construction of ethical regulatory mechanisms. Based on this framework, a seven-step process and case examples for mechanism design are presented, enhancing the practicality of ERF-AI in developing ethical regulatory mechanisms. Future research is expected to explore customization of the framework to remain responsive to emerging AI trends and challenges, supported by empirical studies and rigorous testing for further refinement and expansion.},;Yes;
IT Governance in the Artificial Intelligence Age: Trends and Practices;Abelnica, Kaspars and Petrovskis, Guntis and Vindecs, Agris and Romanovs, Andrejs;2024;2024 IEEE 65th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS);http://doi.org/10.1109/ITMS64072.2024.10741935;The rapid development of artificial intelligence (AI) this decade has created brand new opportunities for companies to improve and rethink their business processes. When adopting AI solutions such as ChatGPT, it is important that current IT governance practices are adapted for this AI revolution. This paper is a literature review that overviews current AI governance trends and future opportunities. Based on this review, the authors have put forth their recommendations for successfully governing AI in business use cases.},;Yes;
Talking past each other? Navigating discourse on ethical AI: Comparing the discourse on ethical AI policy by Big Tech companies and the European Commission;Evers, Cornelia;2024;Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency;https://doi.org/10.1145/3630106.3659013;This study examines the convergence of the European Commission (EC) and Big Tech companies’ (Google and Microsoft) discourse on ‘ethical’ AI through critical discourse analysis and the concept of hegemonic discourse. The paper answers the question to what extent there is a hegemonic discourse on ethical AI between EU policymakers and Big Tech companies and whether this is impacted by the prospect of legally binding legislation, considering the possible impact of the 2021 AI Act Proposal of the European Commission. This analysis is relevant at an inflection point where previous literature notes superficial convergence between the approaches of public and private actors, indicating policy consensus. The scope of analysis however is limited to non-legally binding regulation and lacks regional focus. In the EU, the advent of legally binding AI regulation with the 2021 AI Act (AIA) Proposal marks a critical juncture: with agreement on the AIA in December 2023, ethics standards become part of market entry requirements to the EU Single Market and the underlying differences in approaching Ethical AI will have important ramifications on policy preferences, compliance, enforcement and thought leadership in the domain more broadly. I find that the European discourse on ‘ethical’ AI by the EC and Big Tech companies such as Google and Microsoft is largely hegemonic and depoliticised in non-legally binding settings from 2018-2021 due to shared assumptions on ‘ethical’ AI and absence of significant underlying social and political conflict. It evolves to non-hegemonic and repoliticised discourse through dislocation by the prospect of legally binding regulation, which pushes actors to reveal their genuine policy preferences that bear political and social conflictuality whilst both actor types take an instrumental approach to ethics.;No;Full text not accessible
An Exploratory Diagnosis of Artificial Intelligence Risks for a Responsible Governance.;Teixeira, S\'{o;2022;Proceedings of the 15th International Conference on Theory and Practice of Electronic Governance;https://doi.org/10.1145/3560107.3560298;Our lives have been increasingly filled with technologies that use Artificial Intelligence (AI), whether at home, in public spaces, in social organizations, or in services. Like other technologies, adopting this emerging technology also requires society’s attention to the challenges that may arise from it. The media brought to the public some unexpected results from using these technologies, for example, the unfairness case in the COMPAS system. It became more evident that these technologies can have unintended consequences. In particular, in the public interest domain, these unintended consequences and their origin are a challenge for public policies, governance, and responsible AI. This work aims to identify the technological and ethical risks in data-driven decision systems based on AI and conduct a diagnosis of these risks and their perception. To do that, we use a triangulation of methods. In the first stage, a search on Web of Science has been performed. We consider all the 412 papers. The second stage corresponds to a analysis of experts. The papers have been classified according to the relevance to the topic by the experts. In the third stage, we use the survey method and include risk insights from stage two in our questions. We found 24 concerns which arise from the perspective of the ethical and technological risk perspective. The perception of participants regarding the level of concern they have with the risks of a data-driven system based on AI is high than their perception of society’s concern. Fairness is considered the risk whose perception is more severe. Fairness, Bias, Accountability, Interpretability, and Explainability are considered the most relevant concepts for a responsible AI. Consequently, also the most relevant for responsible governance of AI.;No;Full text not accessible
Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering;Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Zowghi, Didar and Jacquet, Aurelie;2024;ACM Comput. Surv.;https://doi.org/10.1145/3626234;Responsible Artificial Intelligence (RAI) is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of Artificial Intelligence (AI). Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. In addition, significant efforts have been placed at algorithm level rather than system level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize RAI from a system perspective, in this article, we present an RAI Pattern Catalogue based on the results of a multivocal literature review. Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The RAI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and RAI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement RAI.;Yes;
Responsible AI Systems: Who are the Stakeholders?;Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Didar Zowghi, Aurelie Jacquet;2024;https://doi.org/10.1145/3514094.3534187;ACM Computing Surveys;The paper identifies stakeholders in responsible AI systems via literature review and content analysis, structured by ISO 26000:2010 guidance. It provides a long-list to guide selection of ethical guidelines by practitioners.;Yes;
Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems;Korobenko, Daria and Nikiforova, Anastasija and Sharma, Rajesh;2024;Proceedings of the 25th Annual International Conference on Digital Government Research;https://doi.org/10.1145/3657054.3657141;As artificial intelligence (AI) continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Commission has taken a decisive step by reaching a political agreement on the EU AI Act, the world’s first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this challenge, this study conducts a systematic literature review (SLR) with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics.;Yes;
Towards a Responsible AI Metrics Catalogue: A Collection of Metrics for AI Accountability;Xia, Boming and Lu, Qinghua and Zhu, Liming and Lee, Sung Une and Liu, Yue and Xing, Zhenchang;2024;Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI;https://doi.org/10.1145/3644815.3644959;Artificial Intelligence (AI), particularly through the advent of large-scale generative AI (GenAI) models such as Large Language Models (LLMs), has become a transformative element in contemporary technology. While these models have unlocked new possibilities, they simultaneously present significant challenges, such as concerns over data privacy and the propensity to generate misleading or fabricated content. Current frameworks for Responsible AI (RAI) often fall short in providing the granular guidance necessary for tangible application, especially for Accountability---a principle that is pivotal for ensuring transparent and auditable decision-making, bolstering public trust, and meeting increasing regulatory expectations. This study bridges the Accountability gap by introducing our effort towards a comprehensive metrics catalogue, formulated through a systematic multivocal literature review (MLR) that integrates findings from both academic and grey literature. Our catalogue delineates process metrics that underpin procedural integrity, resource metrics that provide necessary tools and frameworks, and product metrics that reflect the outputs of AI systems. This tripartite framework is designed to operationalize Accountability in AI, with a special emphasis on addressing the intricacies of GenAI.;Yes;
Typology of Risks of Generative Text-to-Image Models;Bird, Charlotte and Ungless, Eddie and Kasirzadeh, Atoosa;2023;Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society;https://doi.org/10.1145/3600211.3604722;This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.;Yes;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
;;;;;;;
